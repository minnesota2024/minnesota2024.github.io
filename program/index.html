
<!DOCTYPE html>
<html lang="en">

<head>
  <script type="text/javascript">// <![CDATA[
      function ShowHide(divId)
      {
          if(document.getElementById(divId).style.display == 'none')
          {
              document.getElementById(divId).style.display='block';
          }
          else
          {
              document.getElementById(divId).style.display = 'none';
          }
      }
  // ]]></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <title>
    Program - SPComNet2024
  </title>
   
  <link rel="stylesheet" href="/assets/css/main.css" /> 
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  
</head>
       
<body class="pb-4">
  <header>
    <!-- navigation bar (web only): -->
    <nav class="navbar navbar-expand-md navbar-light bg-light mb-3">
  <div class="container">

    <a class="navbar-brand" href="/"> <font size="+3"><strong>SPComNet2024</strong></font></a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
          
          
            <li class="nav-item active">
              
                <a class="nav-link" href="/program/">
                  Program
                </a>
              
            </li>
          
        
          
            <!-- <li class="nav-item ">
              
                <a class="nav-link" href="/talks/">
                  Talks
                </a>
              
            </li> -->
          
        
          
            <li class="nav-item ">
              
                <a class="nav-link" href="/speakers/">
                  Speakers 
                </a>
              
            </li>
          
        
          
            <li class="nav-item ">
              
                <a class="nav-link" href="/committee/">
                  Committee
                </a>
              
            </li>
          
        
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" id="navbar-dropdown4" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Posters
              </a>
              <div class="dropdown-menu" aria-labelledby="navbar-dropdown4">
                
                  
                    <a class="dropdown-item" href="/call-for-posters.html">
                      Call for Posters
                    </a>
                  
                
                  
                    <!-- <a class="dropdown-item" href="/camera-ready-instructions.html">
                      Camera-ready instructions
                    </a>
                  
                
                  
                    <a class="dropdown-item" href="/video-presentation-instructions">
                      Video recording instructions
                    </a> -->
                  
                
              </div>
            </li>
          
        
          
            <li class="nav-item ">
              
                <a class="nav-link" href="/location/">
                  Location
                </a>
              
            </li>
          
        
          <!--
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" id="navbar-dropdown6" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Archive
              </a>
              <div class="dropdown-menu" aria-labelledby="navbar-dropdown6">
                
                  
                    <a class="dropdown-item" href="/archive/2020">
                      DistributedML'20
                    </a>
                  
                
              </div>
            </li>
          -->
        
      </ul>
    </div>

    
    <div class="navbar-brand navbar-logo mr-0 w-25 text-right overflow-hidden">
      <!-- <a class="" href="https://mlsys.org/">
        
            <img src="/assets/images/mlsys_logo.png" class="navbar-brand-hideable align-middle logo" alt="MLSys 2022" />
        
      </a> -->
      <!--
      <a href="/">
        
          <img class="navbar-brand-hideable align-middle logo" src="/assets/images/xfl_logo.png" alt="CrossFL Workshop" />
        
      </a>
      -->
    </div>
    
 
  </div>
</nav>

    
      <!-- site title (print only): -->
      <div class="container d-none d-print-block mt-4 mb-4">
        <h1 class="display-4">
          MNSPCOM2024
        </h1>
      </div>
    

  </header>
  <main class="container">

    
  
  




  <h1 class="display-5 mb-3">
    
      Program
    
  </h1>

  <p>Preliminary Program for Workshop on May 16-18 2024 </p>


 
  <table>
    <thead>
      <tr>
        <th>Time (CST)</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td colspan="2", style="text-align: center"><strong>Opening Day (May 16)</strong></td>
      </tr>
      <tr>
        <td><strong>6.00 pm</strong></td>
        <td>Welcome reception </td>
      </tr>
      <tr>
        <td colspan="2", style="text-align: center"><strong>Day 1 (May 17)</strong></td>
      </tr>
      <tr>
        <td rowspan="3"><strong>8.30 am -<br> 12.00 noon</strong></td>
        <td><u>Title</u>:<strong>“From High-Order to Filterbanks and Data-driven Signal Processing”</strong>  
          <br>
          <u>Headline speaker</u>: 
          <br>
          <a href="https://asl.epfl.ch/biography/">  <!--CHECK-->
            Ali Sayed
          </a>
          <br>
          <u>Expert Speakers</u>: 
            <ul>
              <li><a href="http://www.ifp.illinois.edu/~yoram">Yoram Bresler</a> </li>
              <li><a href="https://www.eas.caltech.edu/people/ppvnath">P. P. Vaidyanathan</a> </li>
              <li><a href="https://www.weizmann.ac.il/math/yonina/">Yonina Eldar</a> </li>
            </ul>
        </td>
      </tr>
      <tr>
        <td style="text-align: center">Coffee Break: 10.30 am - 11.00 am</td>
      </tr>
      <tr>
        <td>          
          <u>Panelists</u>: 
          <br>
          <a href="https://ece.princeton.edu/people/h-vincent-poor">Vince Poor</a>, <a href="http://dsp.ucsd.edu/home/">Bhaskar Rao</a>, <a href="https://www.eng.auburn.edu/~tugnait/">Jitendra Tugnait</a>, <a href="https://sites.google.com/stonybrook.edu/petardjuric/home">Petar Djuric</a>, <a href="https://scharfcsu.weebly.com">Louis Scharf</a> and <a href="https://evonexus.org/profile/sanyogita-shamsunder/">Sanyogita Shamsunder</a> (coordinators)
          <br>
          <u>Scribes</u>: 
          <br>
          <a href="https://www.linkedin.com/in/brianbaingana">Brian Baingana</a>, <a href="https://www.uta.edu/academics/faculty/profile?username=schizas">Ioannis Schizas</a>, <a href="https://redirect.cs.umbc.edu/~sjkim/">Seung-Jun Kim</a>
        </td>
      </tr>
      <tr>
        <td><strong>12.15 pm -<br> 1.30 pm</strong></td>
        <td>Lunch</td>
      </tr>
      <tr>
        <td rowspan="3"><strong>1.30 pm -<br> 5.00 pm</strong></td>
        <td><u>Title</u>: <strong>“AI-enhanced NextG Wireless Communications”</strong> 
          <br>
          <u>Headline speaker</u>: 
          <br>
          <a href="https://web.stanford.edu/~apaulraj/">Arogyaswami Paulraj</a> - “Key Ideas in Mobile Wireless Technology: 2G to 5G”
          <br>
          <u>Expert Speakers</u>: 
            <ul>
              <li><a href="https://www.kaust.edu.sa/en/study/faculty/mohamed-slim-alouini">Mohamed-Slim Alouini</a> - “What Should 6G Be?”</li>
              <li><a href="https://profheath.org">Robert Heath</a></li>
              <li><a href="https://engineering.uci.edu/users/lee-swindlehurst">Lee Swindlehurst</a></li>
              <li><a href="https://sites.google.com/a/uniroma1.it/sergiobarbarossa/">Sergio Barbarossa</a></li>
            </ul>
        </td>
        </tr>
        <tr>
            <td style="text-align: center">Coffee Break: 3.30 pm - 4.00 pm</td>
        </tr>
        <tr>
          <td> 
          <u>Panelists</u>: 
          <br>
          <a href="https://www.ece.ucdavis.edu/~zding/">Zhi Ding</a>, <a href="https://www.nae.edu/149899/Dr-John-R-Treichler">John Treichler</a>, <a href="http://eceweb1.rutgers.edu/~cspl/">Athina Petropulu</a>, <a href="https://sinelab.tech.cornell.edu">Anna Scaglione</a>, <a href="https://sps.ewi.tudelft.nl/People/bio.php?id=3">Geert Leus</a> and <a href="https://people-ece.vse.gmu.edu/~ztian1/">Zhi Tian</a> (coordinators)
          <br>
          <u>Scribes</u>: 
          <br>
          <a href="https://www.linkedin.com/in/renqiu-rachel-wang-9a5374225?trk=public_profile_browsemap">Renqiu Wang</a>, <a href="https://people.engr.tamu.edu/eserpedin/index.html">Erchin Serpedin</a>, <a href="https://www.linkedin.com/in/luoxiliang">Xiliang Luo</a>
        </td>
      </tr>
      <td><strong>6.00 pm <br> onwards</strong></td>
      <td>Dinner (individually scheduled)</td>
    </tr>
      <tr>
        <td colspan="2", style="text-align: center"><strong>Day 2 (May 18)</strong></td>
      </tr>
      <tr>
        <td rowspan="3"><strong>8.30 am -<br> 12.00 noon</strong></td>
        <td><u>Title</u>:<strong>“AI, Grid, Graphs, and Networks”</strong>  
          <br>
          <u>Headline speaker</u>: 
          <br>
          <a href="http://netlab.caltech.edu">Steven Low</a> <!--CHECK-->
          <br>
          <u>Expert Speakers</u>: 
            <ul>
              <li><a href="http://users.ece.northwestern.edu/~rberry/">Randall Berry</a> </li>
              <li><a href="https://alelab.seas.upenn.edu/alejandro-ribeiro/">Alejandro Ribeiro</a> </li>
              <li><a href="https://www.ece.mcgill.ca/~mcoate/">Mark Coates</a> </li> <!--CHECK-->
            </ul>
        </td>
      </tr>
      <tr>
              <td style="text-align: center">Coffee Break: 10.30 am - 11.00 am</td>
      </tr>
      <tr>
        <td>
          <u>Panelists</u>: 
          <br>
          <a href="https://sites.utexas.edu/haozhu/">Hao Zhu</a>, <a href="http://glaros.dtc.umn.edu/gkhome/index.php">George Karypis</a>, <a href="https://aiem.jhu.edu">Rama Chellappa</a>, <a href="https://viterbi.usc.edu/directory/faculty/Ortega/Antonio">Antonio Ortega</a>, <a href="https://tsc.urjc.es/~amarques/">Antonio G. Marques</a> (coordinator)
          <br>
          <u>Scribes</u>: 
          <br>
          <a href="https://engineering.purdue.edu/~kekatos/">Vassilis Kekatos</a>, <a href="https://ceid.utsa.edu/ngatsis/">Nikos Gatsis</a>, <a href="https://www.colorado.edu/faculty/dallanese">Emiliano Dall Anese</a>
          <!-- <br>
          <u>Remarks</u>:
          <ol>
            <li>Headline speech 50mins+10mins Q&A.</li>
            <li>Expert talks: 16mins+4mins Q&A (one hour total followed by 30mins coffee break)</li>
          </ol> -->
        </td>
      </tr>
    </tbody>
  </table>
<!-- <p>The workshop will take place in Mission Ballroom MR2, Santa Clara Convention Center on the <strong>1st of September 2022</strong>. </p>

 <br>


 <table>
  <thead>
    <tr>
      <th>Time (PST)</th>
      <th>Title</th>
      <th>Speaker</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>8.55 - 9.00</strong></td>
      <td>Opening Remarks </td>
      <td><a href="https://chentianyi1991.github.io/"> Tianyi Chen </a>(RPI) </td>
    </tr>
    <tr>
      <td><strong>9.00 - 9.40</strong></td>
      <td> <strong>Keynote #1</strong> 
        <br>
        ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally! <a onclick="javascript:ShowHide('speaker1')" style="color:#3366BB">[abstract]</a>  <a href="/presentations/ProxSkip-MLSys.pdf"> [slides] </a>
                                  
                                  <div class="mid" id="speaker1" style="display: none;">
                                  <br>
                                  We introduce ProxSkip - a surprisingly simple and provably efficient method for minimizing the sum of a smooth (<img src=/assets/images/f.svg>) and an expensive nonsmooth proximable (<img src=/assets/images/psi.svg>) function. The canonical approach to solving such problems is via the proximal gradient descent (ProxGD) algorithm, which is based on the evaluation of the gradient of <img src=/assets/images/f.svg> and the prox operator of <img src=/assets/images/psi.svg> in each iteration. In this work we are specifically interested in the regime in which the evaluation of prox is costly relative to the evaluation of the gradient, which is the case in many applications. ProxSkip allows for the expensive prox operator to be skipped in most iterations: while its iteration complexity is <img src=/assets/images/itcomp.svg>, where <img src=/assets/images/kappa.svg> is the condition number of <img src=/assets/images/f.svg>, the number of prox evaluations is <img src=/assets/images/numproxeval.svg> only. Our main motivation comes from federated learning, where evaluation of the gradient operator corresponds to taking a local GD step independently on all devices, and evaluation of prox corresponds to (expensive) communication in the form of gradient averaging. In this context, ProxSkip offers an effective acceleration of communication complexity. Unlike other local gradient-type methods, such as FedAvg, SCAFFOLD, S-Local-GD and FedLin, whose theoretical communication complexity is worse than, or at best matching, that of vanilla GD in the heterogeneous data regime, we obtain a provable and large improvement without any heterogeneity-bounding assumptions. </div>
      </td>
      <td>
           <a href="https://richtarik.org/"> Peter Richtarik </a>(KAUST)
      </td>
       
  </tr>
  <tr>
      <td><strong>9.40 - 10.20</strong></td>
      <td> <strong>Keynote #2</strong> 
        <br>
        Three Daunting Challenges of Federated Learning: Privacy Leakage, Label Deficiency, and Resource Constraints <a onclick="javascript:ShowHide('speaker2')" style="color:#3366BB">[abstract]</a>
                                  
                                  <div class="mid" id="speaker2" style="display: none;">
        <br>
                                   Federated learning (FL) has emerged as a promising approach to enable decentralized machine learning directly at the edge, in order to enhance users’ privacy, comply with regulations, and reduce development costs. In this talk, I will provide an overview of FL and highlight three fundamental challenges for landing FL into practice: (1) privacy and security guarantees for FL; (2) label scarcity at the edge; and (3) FL over resource-constrained edge nodes. I will also provide a brief overview of FedML (<a href="https://fedml.ai"> https://fedml.ai </a>), which is a platform that enables zero-code, lightweight, cross-platform, and provably secure federated learning and analytics. </div>
      </td>
      <td>
           <a href="https://www.avestimehr.com/"> Salman Avestimehr </a>(USC)
      </td>
       
  </tr>
  <tr>
      <td><strong>10.20 - 11.00</strong></td>
      <td> <strong>Keynote #3</strong> 
        <br>
        Federated Learning for EdgeAI: New Ideas and Opportunities for Progress <a onclick="javascript:ShowHide('speaker3')" style="color:#3366BB">[abstract]</a>
                                  
                                  <div class="mid" id="speaker3" style="display: none;">
        <br>
                                    EdgeAI aims at the widespread deployment of AI on edge devices. To this end, a critical requirement of future ML systems is to enable on-device automated training and inference in distributed settings, wherever and whenever data, devices, or users are present, without sending the training (possibly sensitive) data to the cloud or incurring long response times.
Starting from these overarching considerations, we consider on-device distributed learning, the hardware it runs on, and their co-design to allow for efficient federated learning and resource-aware deployment on edge devices. We hope to convey the excitement of working in this problem space that brings together topics in ML, optimization, communications, and application-hardware (co-)design. </div>
      </td>
      <td>
           <a href="https://radum.ece.utexas.edu/"> Radu Marculescu </a>(UT Austin)
      </td>
  </tr>
  <tr>
      <td><strong>11.00 - 11.40</strong></td>
      <td> <strong>Keynote #4</strong> 
        <br>
        Model Based Deep Learning with Applications to Federated Learning <a onclick="javascript:ShowHide('speaker4')" style="color:#3366BB">[abstract]</a>
                                  
                                  <div class="mid" id="speaker4" style="display: none;">
        <br>
                                    Deep neural networks provide unprecedented performance gains in many real-world problems in signal and image processing. Despite these gains, the future development and practical deployment of deep networks are hindered by their black-box nature, i.e., a lack of interpretability and the need for very large training sets. On the other hand, signal processing and communications have traditionally relied on classical statistical modeling techniques that utilize mathematical formulations representing the underlying physics, prior information and additional domain knowledge. Simple classical models are useful but sensitive to inaccuracies and may lead to poor performance when real systems display complex or dynamic behavior. Here we introduce various approaches to model based learning which merge parametric models with optimization tools and classical algorithms leading to efficient, interpretable networks from reasonably sized training sets. We then show how model based signal processing can impact federated learning both in terms of communication efficiency and in terms of convergence properties. We will consider examples to image deblurring, super resolution in ultrasound and microscopy, efficient communication systems, and efficient diagnosis of COVID19 using X-ray and ultrasound. </div>
      </td>
      <td>
           <a href="https://www.weizmann.ac.il/math/yonina/"> Yonina Eldar </a>(Weizmann)
      </td>
  </tr>
  <tr>
      <td><strong>11.40 - 13.40</strong></td>
      <td> <strong>Live Demo Session on FedML</strong> 
        <br>
        A tutorial followed by a live demo (an interactive session with participants to run FL in FedML platform)
      </td>
      <td>
           <a href="https://chaoyanghe.com/"> Chaoyang He </a>(<a href="https://fedml.ai/">FedML</a>)
      </td>
  </tr>
  <tr>
      <td><strong>13.40 - 14.20</strong></td>
      <td> <strong>Keynote #5</strong> 
        <br>
       Scalable, Heterogeneity-Aware and Trustworthy Federated Learning <a onclick="javascript:ShowHide('speaker5')" style="color:#3366BB">[abstract]</a>
                                  
                                  <div class="mid" id="speaker5" style="display: none;">
        <br>
        Federated learning has become a popular distributed machine learning paradigm for developing edge AI applications. However, the data residing across the edge devices is intrinsically statistically heterogeneous (i.e., non-IID data distribution) and edge devices usually have limited communication bandwidth to transfer local updates. Such statistical heterogeneity and communication limitation are two major bottlenecks that hinder applying federated learning in practice. In addition, recent works have demonstrated that sharing model updates makes federated learning vulnerable to inference attacks and model poisoning attacks. In this talk, we will present our recent works on novel federated learning frameworks to address the scalability and heterogeneity issues simultaneously. In addition, we will also reveal the essential reason of privacy leakage and model poisoning attacks in federated learning procedures, and provide the defense mechanisms accordingly towards trustworthy federated learning.                         
                                   </div>                       
      </td>
      <td>
           <a href="https://ece.duke.edu/faculty/yiran-chen"> Yiran Chen </a>(Duke)
      </td>
  </tr>
  <tr>
      <td><strong>14.20 - 15.00</strong></td>
      <td> <strong>Keynote #6</strong> 
        <br>
        On Lower Bounds of Distributed Learning with Communication Compression <a onclick="javascript:ShowHide('speaker6')" style="color:#3366BB">[abstract]</a> <a href="/presentations/OptimalCompression.pdf"> [slides] </a>
                                  
                                  <div class="mid" id="speaker6" style="display: none;">
        <br>
                                    There have been many recent works proposing new compressors for various distributed optimization settings. But, all cutting-edge performance analyses come down to one of the only two properties of compressors: unbiasedness or contraction. This leads to a natural question: If we want to improve the convergence rate of distributed optimization with communication compression, should we continue using those properties and focus on how to apply them more cleverly in distributed algorithms, or should we look for new compressor properties? To answer this question, we present theoretical performance lower bounds imposed by those two properties and, then, show that the lower bounds are nearly matched by a method, which works with any compressors satisfying one of those two properties. Hence, future work shall look for a fundamentally new compressor property.

This is joint work with Xinmeng Huang (UPenn), Yiming Chen (Alibaba), and Kun Yuan (Alibaba). </div>
      </td>
      <td>
           <a href="https://www.math.ucla.edu/~wotaoyin/"> Wotao Yin </a>(Alibaba Damo)
      </td>
  </tr>
  <tr>
      <td><strong>15.00 - 17.00</strong></td>
      <td> <strong>Poster Session and Best Student Poster Competition</strong> 
        <br>
        <ol>
        <li>Taejin Kim* (CMU), Shubhranshu Singh (CMU), Nikhil Madaan (CMU), Carlee Joe-Wong (CMU). Grey-Box Defense for Personalized Federated Learning <a href="/abstracts/Abstract2.pdf"> [abstract] </a> <a href="/posters/Poster2.pdf"> [poster] </a></li>
        <li>Ahmed M. Abdelmoniem* (QMUL). Towards Efficient and Practical Federated Learning <a href="/abstracts/Abstract3.pdf"> [abstract] </a> <a href="/posters/Poster3.pdf"> [poster] </a></li>
        <li>Zexi Li* (ZJU),  Chao Wu (ZJU). Towards Effective Clustered Federated Learning: A Peer-to-peer Framework with Adaptive Neighbor Matching <a href="/abstracts/Abstract4.pdf"> [abstract] </a> <a href="/posters/Poster4.pdf"> [poster] </a></li>
        <li>Ziang Song* (JHU), Zhuolong Yu (JHU),  Jingfeng Wu (JHU), Lin Yang (UCLA), Vladimir Braverman (JHU). FLLEdge: Federated Lifelong Learning on Edge Devices <a href="/abstracts/Abstract5.pdf"> [abstract] </a> <a href="/posters/Poster5.pdf"> [poster] </a></li>
        <li>Mohammad Taha Toghani* (Rice),  Cesar Uribe (Rice). Scalable Average Consensus with Compressed Communications <a href="/abstracts/Abstract6.pdf"> [abstract] </a> <a href="/posters/Poster6.pdf"> [poster] </a></li>
        <li>Tian Chunlin (UM),  Li  Li (UM), Zhan Shi (UM), Jun Wang (UM), Cheng-Zhong Xu (UM). HARMONY: Heterogeneity-Aware Hierarchical Management for Federated Learning System <a href="/abstracts/Abstract8.pdf"> [abstract] </a> <a href="/posters/Poster8.pdf"> [poster] </a></li>
        <li>Kunjal Panchal* (UMass),  Hui Guan (UMass). Flow: Fine-grained Personalized Federated Learning through Dynamic Routing <a href="/abstracts/Abstract9.pdf"> [abstract] </a> <a href="/posters/Poster9.pdf"> [poster] </a></li>
        <li>Yongbo Yu* (GMU),  Fuxun Yu (GMU),  Zirui Xu (GMU). Powering Multi-Task Federated Learning with Competitive GPU Resource Sharing <a href="/abstracts/Abstract10.pdf"> [abstract] </a> <a href="/posters/Poster10.pdf"> [poster] </a></li>
        <li>Yuchen Zeng* (UW Madison), Hongxu Chen (UW Madison), Kangwook Lee (UW Madison). Improving Fairness via Federated Learning <a href="/abstracts/Abstract12.pdf"> [abstract] </a> <a href="/posters/Poster12.pdf"> [poster] </a></li>
        <li>Sepehr Delpazir* (RIT), Ali Anwar (IBM Research), Nathalie Baracaldo (IBM Research), Khalil Al-Hussaeni (RIT). Understanding Resource Requirements of FL Algorithms <a href="/abstracts/Abstract13.pdf"> [abstract] </a> <a href="/posters/Poster13.pdf"> [poster] </a></li>
        <li>Xinchi Qiu* (Cambridge), Javier Fernandez-Marques (Samsung AI), Pedro Gusmao (Cambridge), Yan Gao (Newcastle), Titouan Parcollet (Oxford), Nicholas Lane (Cambridge and Samsung AI). On-device Training with Local Sparsity for FL <a href="/abstracts/Abstract14.pdf"> [abstract] </a> <a href="/posters/Poster14.pdf"> [poster] </a></li>
        <li>Shenghong Dai* (UW Madison), Kangwook Lee (UW Madison), Suman Banerjee (UW Madison). Dynamic Decentralized Federated Learning <a href="/abstracts/Abstract17.pdf"> [abstract] </a> <a href="/posters/Poster17.pdf"> [poster] </a></li>
        <li>Karthik Pansetty* (CMU), Yuhang Yao (CMU), Mohammad Mahdi Kamani (Wyze), Carlee Joe-Wong (CMU). Personalized Federated Graph Learning <a href="/abstracts/Abstract18.pdf"> [abstract] </a> <a href="/posters/Poster18.pdf"> [poster] </a></li>
      </ol>
      <p style="color:red"><strong>Best Paper Award:</strong></p>
      <ul>
        <li>Yongbo Yu* (GMU), Fuxun Yu (GMU), Zirui Xu (GMU), Xiang Chen (GMU). Powering Multi-Task Federated Learning with Competitive GPU Resource Sharing</li>
        <li>Taejin Kim* (CMU), Shubhranshu Singh (CMU), Nikhil Madaan (CMU), Carlee Joe-Wong (CMU). Grey-Box Defense for Personalized Federated Learning</li>
      </ul> 
      </td>
      <td>
           
      </td>
  <tr>
    <td><strong>17.00 - 17.05</strong></td>
    <td>Closing Remarks </td>
    <td>TPC </td>
  </tr>
  </tr>
  </tbody>
</table>
     
 <br> 
 <p><font size="+2"><strong>Sponsors</strong></font><p>
  <a class="" href="https://airc.rpi.edu/">

    <img src="./../assets/images/rpi_ibm_airc_logo.png" class="align-middle logo" alt="RPI-IBM AIRC" width="390px" height="42px"/>

  </a> 
  <span style="padding-left:40px">
  <a class="" href="https://fedml.ai/">

    <img src="./../assets/images/fedml_logo.png" class="align-middle logo" alt="FedML" width="220px" height="60px"/>

  </a>  -->
    
  

 

  </main>
  <footer class="container"></footer>

  <script src="/assets/js/main.js"></script>

</body>
</html>

